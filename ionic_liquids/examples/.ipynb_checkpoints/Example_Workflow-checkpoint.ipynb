{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of main.py in the ionic_liquids folder.  I will first have to import the libraries that are necessary to run this program, including train_test_split that allows for splitting datasets into training sets and test sets necessary to run machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, I will utilize the following filename, machine learning model, and  directory name to save the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = 'inputdata2.xlsx'\n",
    "MODEL = 'mlp_regressor'\n",
    "DIRNAME = 'my_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step prepares the data to be read in the machine_learning methods. First, we need to get the data into a readable form and parse, if necessary. In our case, we need to parse the values and errors in the last column of the FILENAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inputdata2.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5c10f97db2d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-5c10f97db2d4>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfiletype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'xls'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xlsx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Filetype not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SarahsAdventure/miniconda3/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheetname, header, skiprows, skip_footer, index_col, names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, true_values, false_values, engine, squeeze, **kwds)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     return io._parse_excel(\n",
      "\u001b[0;32m/Users/SarahsAdventure/miniconda3/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m/Users/SarahsAdventure/miniconda3/lib/python3.5/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'inputdata2.xlsx'"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Reads data in from given file to Pandas DataFrame\n",
    "\n",
    "    Inputs\n",
    "    -------\n",
    "    filename : string of path to file\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    df : Pandas DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    cols = filename.split('.')\n",
    "    name = cols[0]\n",
    "    filetype = cols[1]\n",
    "    if (filetype == 'csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "    elif (filetype in ['xls', 'xlsx']):\n",
    "        df = pd.read_excel(filename)\n",
    "    else:\n",
    "        raise ValueError('Filetype not supported')\n",
    "\n",
    "    # clean the data if necessary\n",
    "    df['EC_value'], df['EC_error'] = zip(*df['ELE_COD'].map(lambda x: x.split('Â±')))\n",
    "    df = df.drop('EC_error', 1)\n",
    "    df = df.drop('ELE_COD', 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = read_data(FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we will create a X matrix and y vector that are send to the molecular descriptor function in utils.py. The X matrix will hold all of our inputs for the machine learning whereas y vector will be the actual electronic conductivity values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-16f9291a4afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprenorm_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmolecular_descriptors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def molecular_descriptors(data):\n",
    "    \"\"\"\n",
    "    Use RDKit to prepare the molecular descriptor\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    data: dataframe, cleaned csv data\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    prenorm_X: normalized input features\n",
    "    Y: experimental electrical conductivity\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n = data.shape[0]\n",
    "    # Choose which molecular descriptor we want\n",
    "    list_of_descriptors = ['NumHeteroatoms', 'ExactMolWt',\n",
    "        'NOCount', 'NumHDonors',\n",
    "        'RingCount', 'NumAromaticRings', \n",
    "        'NumSaturatedRings', 'NumAliphaticRings']\n",
    "    # Get the molecular descriptors and their dimension\n",
    "    calc = Calculator(list_of_descriptors)\n",
    "    D = len(list_of_descriptors)\n",
    "    d = len(list_of_descriptors)*2 + 4\n",
    "\n",
    "    Y = data['EC_value']\n",
    "    X = np.zeros((n, d))\n",
    "    X[:, -3] = data['T']\n",
    "    X[:, -2] = data['P']\n",
    "    X[:, -1] = data['MOLFRC_A']\n",
    "    for i in range(n):\n",
    "        A = Chem.MolFromSmiles(data['A'][i])\n",
    "        B = Chem.MolFromSmiles(data['B'][i])\n",
    "        X[i][:D]    = calc.CalcDescriptors(A)\n",
    "        X[i][D:2*D] = calc.CalcDescriptors(B)\n",
    "\n",
    "    prenorm_X = pd.DataFrame(X,columns=['NUM', 'NumHeteroatoms_A', \n",
    "        'MolWt_A', 'NOCount_A','NumHDonors_A', \n",
    "        'RingCount_A', 'NumAromaticRings_A', \n",
    "        'NumSaturatedRings_A',\n",
    "        'NumAliphaticRings_A', \n",
    "        'NumHeteroatoms_B', 'MolWt_B', \n",
    "        'NOCount_B', 'NumHDonors_B',\n",
    "        'RingCount_B', 'NumAromaticRings_B', \n",
    "        'NumSaturatedRings_B', \n",
    "        'NumAliphaticRings_B',\n",
    "        'T', 'P', 'MOLFRC_A'])\n",
    "\n",
    "    prenorm_X = prenorm_X.drop('NumAliphaticRings_A', 1)\n",
    "    prenorm_X = prenorm_X.drop('NumAliphaticRings_B', 1)\n",
    "\n",
    "    return prenorm_X, Y\n",
    "\n",
    "X, y = molecular_descriptors(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can prepare our testing and training data set for the machine learning calling using train_test_split, a function called from sklearn module of python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Followingly, the program will normalize the testing data using the training data set. This will also provide us with the mean value and standard deviation of X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization(data, means=None, stdevs=None):\n",
    "    \"\"\"\n",
    "    Normalizes the data using the means and standard\n",
    "    deviations given, calculating them otherwise.\n",
    "    Returns the means and standard deviations of columns.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    data : Pandas DataFrame\n",
    "    means : optional numpy argument of column means\n",
    "    stdevs : optional numpy argument of column st. devs\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    normed : the normalized DataFrame\n",
    "    means : the numpy row vector of column means\n",
    "    stdevs : the numpy row vector of column st. devs\n",
    "\n",
    "    \"\"\"\n",
    "    cols = data.columns\n",
    "    data = data.values\n",
    "\n",
    "    if (means is None) or (stdevs is None):\n",
    "        means = np.mean(data, axis=0)\n",
    "        stdevs = np.std(data, axis=0, ddof=1)\n",
    "    else:\n",
    "        means = np.array(means)\n",
    "        stdevs = np.array(stdevs)\n",
    "\n",
    "    # handle special case of one row\n",
    "    if (len(data.shape) == 1) or (data.shape[0] == 1):\n",
    "        for i in range(len(data)):\n",
    "            data[i] = (data[i] - means[i]) / stdevs[i]\n",
    "    else: \n",
    "        for i in range(data.shape[1]):\n",
    "            data[:,i] = (data[:,i] - means[i]*np.ones(data.shape[0])) / stdevs[i]\n",
    "\n",
    "    normed = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "    return normed, means, stdevs\n",
    "\n",
    "X_train, X_mean, X_std = normalization(X_train)\n",
    "X_test, trash, trash = normalization(X_test, X_mean, X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We coded three models into our program:  MLP_regressor, LASSO, and SVR.  Each of these models are  well documented in  sklearn, a library in  python.  *In the actual program, you can use all three models, but for the purpose of this example, we chose mlp_regressor.* The ValueError will only raise if you do not use one of the three models. A good example is if you were to change the MODEL used to 'MLP_classifier'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (MODEL.lower() == 'mlp_regressor'):\n",
    "    obj = methods.do_MLP_regressor(X_train, y_train)\n",
    "elif (MODEL.lower() == 'lasso'):\n",
    "    obj = methods.do_lasso(X_train, y_train)\n",
    "elif (MODEL.lower() == 'svr'):\n",
    "    obj = methods.do_svr(X_train, y_train)\n",
    "else:\n",
    "    raise ValueError(\"Model not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the method is called , it will be saved to an objective.  This objective is saved along with the mean and standard deviation and the training set in the directory, named  DIRNAME. This step is not as important for the workflow but vital to the success of the graphical user interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(obj, X_mean, X_stdev, X=None, y=None, dirname='default'):\n",
    "    \"\"\"\n",
    "    Save the trained regressor model to the file\n",
    "\n",
    "    Input\n",
    "    ------\n",
    "    obj: model object\n",
    "    X_mean : mean for each column of training X\n",
    "    X_stdev : stdev for each column of training X\n",
    "    X : Predictor matrix\n",
    "    y : Response vector\n",
    "    dirname : the directory to save contents\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if (dirname == 'default'):\n",
    "        timestamp = str(datetime.now())[:19]\n",
    "        dirname = 'model_'+timestamp.replace(' ', '_')\n",
    "    else:\n",
    "        pass\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    filename = dirname + '/model.pkl'\n",
    "    joblib.dump(obj, filename)\n",
    "\n",
    "    joblib.dump(X_mean, dirname+'/X_mean.pkl')\n",
    "    joblib.dump(X_stdev, dirname+'/X_stdev.pkl')\n",
    "\n",
    "    if (X is not None):\n",
    "        filename = dirname + '/X_data.pkl'\n",
    "        joblib.dump(X, filename)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if (y is not None):\n",
    "        filename = dirname + '/y_data.pkl'\n",
    "        joblib.dump(y, filename)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return\n",
    "\n",
    "save_model(obj, X_mean, X_std, X_train, y_train, dirname=DIRNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the experimental values will be scatter plotted against the predicted values. We will use the parity_plot to do so. plt.show()  function will just allow the plot to show up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parity_plot(y_pred, y_act):\n",
    "    \"\"\"\n",
    "    Creates a parity plot\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    y_pred : predicted values from the model\n",
    "    y_act : 'true' (actual) values\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    fig : matplotlib figure\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=FIG_SIZE)\n",
    "    plt.scatter(y_act, y_pred)\n",
    "    plt.plot([y_act.min(), y_act.max()], [y_act.min(), y_act.max()],\n",
    "             lw=4, color='r')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "\n",
    "    return fig\n",
    "\n",
    "my_plot = parity_plot(y_train, obj.predict(X_train))\n",
    "plt.show(my_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to look at the other examples that will be more explicit about the functions.  I hope you enjoy our package and use it to fit your needs!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
